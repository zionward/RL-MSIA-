{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image = image/255.0\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experience Replay\n",
    "class Replay():\n",
    "    def __init__(self, buff_size = 100, n = 32):\n",
    "        self.buffer = deque(maxlen = buff_size)\n",
    "        self.n = n\n",
    "        \n",
    "    def add(self, exp):\n",
    "        self.buffer.append(exp)\n",
    "        \n",
    "    def sample(self):\n",
    "        output = list(random.sample(self.buffer, self.n)).copy()\n",
    "        return output\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History():\n",
    "    def __init__(self, history_size = 4):\n",
    "        self.buffer = deque(maxlen = history_size)\n",
    "        \n",
    "    def add(self, frame):\n",
    "        self.buffer.append(frame)\n",
    "        \n",
    "    def output(self):\n",
    "        buffer_list = list(self.buffer)\n",
    "        output = np.stack(buffer_list, axis = -1).copy()\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, lr = 0.01, action_size = 4, history_size = 4, y_shape = 80, name = 'DQN'):\n",
    "        self.action_size = action_size\n",
    "        self.epsilon_decay = 0.01\n",
    "        self.epsilon_step = 0\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.scope = name\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, y_shape, 80, history_size], name = \"inputs\")\n",
    "            self.expected_rewards_  = tf.placeholder(tf.float32,[None, ], name = \"expected_rewards\")\n",
    "            self.Q = tf.placeholder(tf.float32,[None,], name = \"action_pred\")\n",
    "            self.actions_ = tf.placeholder(tf.float32,shape = [None, action_size], name = \"actions\" )\n",
    "            self.avg_max_Q_ = tf.placeholder(tf.float32, name=\"avg_max_Q\")\n",
    "            self.reward_ = tf.placeholder(tf.float32, name=\"reward\")\n",
    "            self.epoch_loss_ = tf.placeholder(tf.float32, name = \"epoch_loss\")\n",
    "            \n",
    "            #CNN\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(\n",
    "                inputs = self.inputs_,\n",
    "                filters = 16,\n",
    "                kernel_size = [8,8],\n",
    "                strides = [4,4],\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                activation = tf.nn.relu\n",
    "            )\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(\n",
    "                inputs = self.conv1,\n",
    "                filters = 8,\n",
    "                kernel_size = [4,4],\n",
    "                strides = [2,2],\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                activation = tf.nn.relu\n",
    "            )\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv2)\n",
    "            \n",
    "            self.fc1 = tf.layers.dense(self.flatten,units = 512, activation = tf.nn.relu,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name=\"fc1\")\n",
    "            \n",
    "            self.preds = tf.layers.dense(\n",
    "                self.fc1, units=action_size,\n",
    "                kernel_initializer=tf.variance_scaling_initializer(),\n",
    "                activation=tf.nn.relu)\n",
    "        \n",
    "        with tf.variable_scope(\"Q\"): #multiply the output by two\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.preds, self.actions_), axis=1)\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.losses.huber_loss(self.expected_rewards_, self.Q))\n",
    "            \n",
    "        with tf.variable_scope(\"train\"):\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(lr, momentum=0.95, epsilon=0.01)\n",
    "            self.train = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "        with tf.variable_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"epoch_loss\", self.epoch_loss_)\n",
    "            tf.summary.scalar(\"avg_max_Q\", self.avg_max_Q_)\n",
    "            tf.summary.scalar(\"reward\", self.reward_)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "            \n",
    "    def predict(self, sess, state):\n",
    "        result = sess.run(self.preds, feed_dict={self.inputs_: state})\n",
    "        return result\n",
    "    \n",
    "    def update(self, sess, state, action, expected_rewards):\n",
    "        feed_dict = {self.inputs_: state, \n",
    "                    self.actions_: action, \n",
    "                    self.expected_rewards_: expected_rewards}\n",
    "        loss = sess.run([self.loss, self.train], feed_dict=feed_dict)\n",
    "        return loss\n",
    "            \n",
    "    def predict_next(self, sess, state):\n",
    "        epsilon = self.epsilon_min + (1-self.epsilon_min) * np.exp(-self.epsilon_decay*self.epsilon_step)\n",
    "        sample = np.random.rand()\n",
    "        \n",
    "        if sample < epsilon:\n",
    "            action = np.random.randint(0,self.action_size)\n",
    "        else: \n",
    "            action = np.argmax(self.predict(sess, [state]))\n",
    "        self.epsilon_step += 1\n",
    "        return action\n",
    "    \n",
    "    def summary(self, sess, loss, avg_max_Q, reward):\n",
    "        summary = sess.run(self.summary_op, feed_dict = {self.epoch_loss_: loss, self.avg_max_Q_: avg_max_Q, self.reward_ :reward})\n",
    "        return loss, summary\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_copier(sess, q_network, target_network):\n",
    "    \n",
    "    # Get and sort parameters\n",
    "    q_params = [t for t in tf.trainable_variables() if t.name.startswith(q_network.scope)]\n",
    "    q_params = sorted(q_params, key=lambda v: v.name)\n",
    "    t_params = [t for t in tf.trainable_variables() if t.name.startswith(target_network.scope)]\n",
    "    t_params = sorted(t_params, key=lambda v: v.name)\n",
    "    \n",
    "    # Assign Q-Parameters to Target Network\n",
    "    updates = []\n",
    "    for q, t in zip(q_params, t_params):\n",
    "        update = t.assign(q)\n",
    "        updates.append(update)\n",
    "    \n",
    "    sess.run(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziyingwang/anaconda3/envs/dl/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "target_updates = 100\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "obs = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-14-5bfffb548488>:26: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /Users/ziyingwang/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-14-5bfffb548488>:38: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-5bfffb548488>:42: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/ziyingwang/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:448: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/ziyingwang/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "QNetwork = DQN(name='QNetwork')\n",
    "target = DQN(name='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "buffer = Replay(buff_size = 1000)\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #count for reseting network\n",
    "    count = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    state_pro = preprocess(state)\n",
    "    for i in range(5):\n",
    "        history.add(state_pro)\n",
    "        \n",
    "    for i in range(1000):\n",
    "        old_hist = history.output()\n",
    "        action = QNetwork.predict_next(sess,old_hist)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        new_pro = preprocess(new_state)\n",
    "        history.add(new_pro)\n",
    "        new_output = history.output()\n",
    "        \n",
    "        one_hot_action = np.zeros(4)\n",
    "        one_hot_action[action] = 1\n",
    "        \n",
    "        buffer.add([[old_hist, one_hot_action, new_output, reward, done]])\n",
    "        \n",
    "        #if done, update the history\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state_pro = preprocess(state)\n",
    "            for i in range(5):\n",
    "                history.add(prepro)\n",
    "                \n",
    "    for epoch in range(n_epochs):\n",
    "        result = []\n",
    "        total_reward = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state_pro = preprocess(state)\n",
    "        for i in range(5):\n",
    "            history.add(state_pro)\n",
    "            \n",
    "        while True:\n",
    "            old_hist = history.output()\n",
    "            action = QNetwork.predict_next(sess, old_hist)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            new_pro = preprocess(new_state)\n",
    "            history.add(new_state)\n",
    "            new_hist = history.output()\n",
    "            \n",
    "            one_hot_action = np.zeros(4)\n",
    "            one_hot_action[action] = 1\n",
    "            \n",
    "            buffer.add([old_hist, one_hot_action, new_hist, reward, done])\n",
    "            \n",
    "            sample = np.array(buffer.sample())\n",
    "            state_2, action_2, new_state_2, reward_2, done_2 = zip(*sample)\n",
    "            # Find max Q-Value per batch for progress\n",
    "            Q_preds = sess.run(QNetwork.Q, \n",
    "                                feed_dict={QNetwork.inputs_: state_2,\n",
    "                                QNetwork.actions_: action_2})\n",
    "            result.append(np.max(Q_preds))\n",
    "            \n",
    "            # Q-Network\n",
    "            Total_preds = []\n",
    "            Total_preds_batch = target.predict(sess, new_state_2)\n",
    "            for i in range(batch_size):\n",
    "                terminal = done_2[i]\n",
    "                if terminal:\n",
    "                    Total_preds.append(reward_2[i])\n",
    "                else:\n",
    "                    Total_preds.append(reward_2[i] + discount_rate * np.max(Total_preds_batch[i]))\n",
    "\n",
    "            # Update Q-Network\n",
    "            loss, _ = QNetwork.update(sess, state_2, action_2, Total_preds) \n",
    "            break\n",
    "        else:\n",
    "            total_reward = total_reward + reward\n",
    "            \n",
    "        count += 1\n",
    "        if count % target_updates == 0:\n",
    "            param_copier(sess, QNetwork, target)\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
